---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
  interval: 1h
  values:
    cleanPrometheusOperatorObjectNames: true

    # Disable kube-proxy monitoring (Cilium runs with kubeProxyReplacement enabled)
    kubeProxy:
      enabled: false

    # Disable etcd monitoring (Talos doesn't expose etcd metrics through standard endpoints)
    kubeEtcd:
      enabled: false

    # Default alert rules
    defaultRules:
      rules:
        kubeProxy: false

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    alertmanager:
      config:
        global:
          resolve_timeout: 5m
        inhibit_rules:
          - source_matchers:
              - 'severity = critical'
            target_matchers:
              - 'severity =~ warning|info'
            equal:
              - 'namespace'
              - 'alertname'
          - source_matchers:
              - 'severity = warning'
            target_matchers:
              - 'severity = info'
            equal:
              - 'namespace'
              - 'alertname'
          - source_matchers:
              - 'alertname = InfoInhibitor'
            target_matchers:
              - 'severity = info'
            equal:
              - 'namespace'
          - target_matchers:
              - 'alertname = InfoInhibitor'
        route:
          group_by:
            - alertname
            - namespace
          group_wait: 1m
          group_interval: 5m
          repeat_interval: 12h
          receiver: telegram
          routes:
            - matchers:
                - alertname="Watchdog"
              receiver: deadman-webhook
              repeat_interval: 5m
            - matchers:
                - alertname=~"KubePod.*"
              group_by:
                - alertname
                - namespace
                - severity
                - cluster
                - deployment
                - statefulset
                - daemonset
                - node
                - pod
              receiver: telegram
        receivers:
          - name: "null"
          - name: deadman-webhook
            webhook_configs:
              - url: "${SECRET_DEADMAN_WORKER_URL}/ping/watchdog-homelab"
                send_resolved: false
                http_config:
                  bearer_token_file: /etc/alertmanager/secrets/alertmanager-deadman-secret/bearer-token
          - name: telegram
            telegram_configs:
              - bot_token_file: /etc/alertmanager/secrets/alertmanager-telegram-secret/bot-token
                chat_id: ${SECRET_TELEGRAM_CHAT_ID}
                parse_mode: HTML
                message: |-
                  {{- range .Alerts -}}
                  {{- if eq .Status "firing" }}ðŸ”¥{{ else }}âœ…{{ end }} <b>{{ .Status | toUpper }}</b>: {{ .Labels.alertname }}
                  {{- if .Labels.severity }}{{ "\n" }}severity: {{ .Labels.severity }}{{- end }}
                  {{- if .Annotations.summary }}{{ "\n" }}<i>{{ .Annotations.summary }}</i>{{- end }}
                  {{- if .Annotations.description }}{{ "\n" }}{{ .Annotations.description }}{{- end }}
                  {{- if or .Labels.cluster .Labels.namespace .Labels.pod .Labels.container .Labels.deployment .Labels.statefulset .Labels.daemonset .Labels.node }}
                  {{ "\n\n" }}<b>Context</b>
                  {{- if .Labels.cluster }}{{ "\n" }}cluster: {{ .Labels.cluster }}{{- end }}
                  {{- if .Labels.namespace }}{{ "\n" }}ns: {{ .Labels.namespace }}{{- end }}
                  {{- if .Labels.pod }}{{ "\n" }}pod: {{ .Labels.pod }}{{- end }}
                  {{- if .Labels.container }}{{ "\n" }}container: {{ .Labels.container }}{{- end }}
                  {{- if .Labels.deployment }}{{ "\n" }}deploy: {{ .Labels.deployment }}{{- end }}
                  {{- if .Labels.statefulset }}{{ "\n" }}sts: {{ .Labels.statefulset }}{{- end }}
                  {{- if .Labels.daemonset }}{{ "\n" }}ds: {{ .Labels.daemonset }}{{- end }}
                  {{- if .Labels.node }}{{ "\n" }}node: {{ .Labels.node }}{{- end }}
                  {{- end }}
                  {{- if or .Annotations.runbook_url .GeneratorURL $.ExternalURL }}
                  {{ "\n\n" }}<b>Links</b>{{ "\n" }}
                  {{- if .Annotations.runbook_url }}<a href="{{ .Annotations.runbook_url }}">Runbook</a>{{- end }}
                  {{- if .GeneratorURL }}{{ if .Annotations.runbook_url }} | {{ end }}<a href="{{ .GeneratorURL }}">Graph</a>{{- end }}
                  {{- if $.ExternalURL }}{{ if or .Annotations.runbook_url .GeneratorURL }} | {{ end }}<a href="{{ $.ExternalURL }}">Alertmanager</a>{{- end }}
                  {{- end }}
                  {{- end }}
        templates:
          - '/etc/alertmanager/config/*.tmpl'
      alertmanagerSpec:
        externalUrl: "https://alertmanager.${SECRET_DOMAIN}"
        secrets:
          - alertmanager-telegram-secret
          - alertmanager-deadman-secret
        resources:
          requests:
            memory: 64Mi
            cpu: 10m
          limits:
            memory: 128Mi
            cpu: 100m
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: openebs-hostpath
              resources:
                requests:
                  storage: 1Gi

    # Prometheus
    prometheus:
      prometheusSpec:
        externalUrl: "https://prometheus.${SECRET_DOMAIN}"
        externalLabels:
          cluster: homelab
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        retention: 14d
        retentionSize: 50GB
        resources:
          requests:
            memory: 800Mi
            cpu: 150m
          limits:
            memory: 1536Mi
            cpu: 1000m
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: openebs-hostpath
              resources:
                requests:
                  storage: 50Gi

    # Grafana with Authentik OAuth
    grafana:
      enabled: true
      resources:
        requests:
          memory: 350Mi
          cpu: 25m
        limits:
          memory: 700Mi
          cpu: 200m
      persistence:
        enabled: true
        storageClassName: openebs-hostpath
      initChownData:
        enabled: false
      admin:
        existingSecret: grafana-admin-credentials
        userKey: username
        passwordKey: password
      grafana.ini:
        analytics:
          reporting_enabled: false
          feedback_links_enabled: false
          check_for_updates: false
          check_for_plugin_updates: false
        server:
          root_url: "https://grafana.${SECRET_DOMAIN}"
        auth:
          disable_login_form: true
          disable_signout_menu: true
        auth.generic_oauth:
          enabled: true
          name: Authentik
          scopes: "openid profile email groups"
          auth_url: "https://authentik.${SECRET_DOMAIN}/application/o/authorize/"
          token_url: "https://authentik.${SECRET_DOMAIN}/application/o/token/"
          api_url: "https://authentik.${SECRET_DOMAIN}/application/o/userinfo/"
          login_attribute_path: preferred_username
          email_attribute_path: email
          groups_attribute_path: groups
          role_attribute_path: contains(groups[*], 'homeserver_admin') && 'Admin'
          allowed_groups: homeserver_admin
          allow_sign_up: true
          auto_login: true
      envValueFrom:
        GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
          secretKeyRef:
            name: grafana-authentik-oauth-client-secret
            key: client-id
        GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
          secretKeyRef:
            name: grafana-authentik-oauth-client-secret
            key: client-secret
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          url: "http://loki-gateway.monitoring.svc.cluster.local:80"
          isDefault: false
          editable: false
      sidecar:
        datasources:
          defaultDatasourceEnabled: true
          isDefaultDatasource: true

    # Subcharts
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      resources:
        requests:
          memory: 32Mi
          cpu: 10m
        limits:
          memory: 64Mi
          cpu: 100m
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      resources:
        requests:
          memory: 128Mi
          cpu: 10m
        limits:
          memory: 256Mi
          cpu: 100m

    # Custom alerts
    additionalPrometheusRulesMap:
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: >-
                    Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }}
                    has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: >-
                  (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1)
                  and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
      dockerhub-rules:
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: >-
                    There are {{ $value }} containers pulling from Dockerhub.
                    This may lead to rate limiting.
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
